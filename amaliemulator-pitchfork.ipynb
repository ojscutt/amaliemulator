{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea59cbfc-2068-4871-909a-00d34c580cae",
   "metadata": {},
   "source": [
    "# amaliemulator\n",
    "notebook for getting Amalie started with training neural nets on her grid of stellar models. Likely to be a mess of pasted and pseudo code (sorry Amalie!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e344be-c24c-4502-8f1e-b6fa34b056de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### misc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "#### graphical\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "\n",
    "#### ML\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "##### poke gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\") \n",
    "\n",
    "gpu0usage = tf.config.experimental.get_memory_info(\"GPU:0\")[\"current\"]\n",
    "\n",
    "print(\"Current GPU usage:\\n\"\n",
    "     + \" - GPU0: \" + str(gpu0usage) + \"B\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9903b58f-e2c8-4fe0-bb34-441730d1ec7a",
   "metadata": {},
   "source": [
    "## data prep\n",
    "usually called 'data augmentation' in the ML community, but I think referees will have questions if you say \"we augmented the data to make our network train better\" - this is where we get our data ready for a neural network to start training.\\\n",
    "usually consists of:\n",
    "- check and clean data to remove NaNs etc\n",
    "- define our inputs and outputs for training\n",
    "- scale data for better network training\n",
    "- split into our *train*, *validation*, and *test* sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8707c78-ed3f-444f-a1d7-74e3ebb2caa9",
   "metadata": {},
   "source": [
    "### inps, outs and normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cdc759-1494-4a5b-b9ff-4227989596b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_hdf('/home/oxs235/datastorage/repos_data/ojscutt/pitchfork/data/barbie_nu.h5', key='df') ## edit for your grid!!\n",
    "\n",
    "#### define inputs\n",
    "inputs = ['initial_mass', 'initial_Zinit', 'initial_Yinit', 'initial_MLT', 'star_age']\n",
    "\n",
    "#### define outputs\n",
    "classical_outputs = ['radius', 'luminosity', 'surface_Z']\n",
    "astero_outputs = [f'nu_0_{i+1}' for i in range(15,25)] # 10 modes for now\n",
    "\n",
    "outputs = classical_outputs+astero_outputs\n",
    "\n",
    "df = df_full[inputs+outputs]\n",
    "\n",
    "df_norm = (df - df.min())/(df.max() - df.min())\n",
    "\n",
    "## check df_norm.describe looks reasonable (min=0, max=1):\n",
    "df_norm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf18bde-5e67-44b3-ae19-94b9edcf570d",
   "metadata": {},
   "source": [
    "### split into train/validate/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cae678-f6f8-4a53-9876-ae934d8ecef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### train/test split with seed \n",
    "seed = 42\n",
    "\n",
    "df_train = df_norm.sample(frac=0.95, random_state=seed)\n",
    "df_test = df_norm.drop(df_train.index)\n",
    "\n",
    "df_train_inputs, df_val_inputs, df_train_outputs, df_val_outputs = sklearn.model_selection.train_test_split(df_train[inputs],df_train[outputs], test_size = 0.05, random_state=seed)\n",
    "\n",
    "print(\"Training set: \", len(df_train_inputs))\n",
    "print(\"Validation set: \", len(df_val_inputs))\n",
    "print(\"Test set: \", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0e581b-e8b1-4d5d-ac21-df18cef213dd",
   "metadata": {},
   "source": [
    "## training our network\n",
    "here we'll use our *train* and *validation* sets to train our neural network and check for overfitting\\\n",
    "I'm using the tensorflow functional API here - while this is a little harder to understand than the sequential API, it's more versatile and will let us do fun things like branching the neural network architecture later on!\\\n",
    "This time I've dropped in my 'pitchfork' branching neural network architecture. Let me know if you need any explainations for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee02cd-9214-426a-9b8b-ac5fae95fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        ________\n",
    "_______/\n",
    "       \\________\n",
    "| stem | tines |\n",
    "\n",
    "\"\"\"\n",
    "######## define architecture:\n",
    "model_name = 'pitchfork'\n",
    "\n",
    "stem_dense_layers = 2 #number of dense layers in stem\n",
    "stem_dense_units = 128 #neurons per dense layer in stem\n",
    "\n",
    "ctine_dense_layers = 2 #number of dense layers in 'classicals' tine\n",
    "ctine_dense_units = 64 #neurons per dense layer in 'classicals' tine\n",
    "\n",
    "atine_dense_layers = 6 #number of dense layers in 'asteroseismic' tine\n",
    "atine_dense_units = 128 #neurons per dense layer in 'asteroseismic' tine\n",
    "\n",
    "\n",
    "######## stem\n",
    "#### input\n",
    "stem_input = keras.Input(shape=(len(inputs),))\n",
    "\n",
    "for stem_dense_layer in range(stem_dense_layers):\n",
    "    if stem_dense_layer == 0:\n",
    "        stem = layers.Dense(stem_dense_units, activation='elu')(stem_input)\n",
    "    else:\n",
    "        stem = layers.Dense(stem_dense_units, activation='elu')(stem)\n",
    "\n",
    "######## classical tine\n",
    "#### dense layers\n",
    "for ctine_dense_layer in range(ctine_dense_layers):\n",
    "    if ctine_dense_layer == 0:\n",
    "        ctine = layers.Dense(ctine_dense_units, activation='elu')(stem)\n",
    "    else:\n",
    "        ctine = layers.Dense(ctine_dense_units, activation='elu')(ctine)\n",
    "\n",
    "#### output\n",
    "ctine_out = layers.Dense(len(classical_outputs),name='classical_outs')(ctine)\n",
    "\n",
    "\n",
    "######## astero tine\n",
    "#### dense layers\n",
    "for atine_dense_layer in range(atine_dense_layers):\n",
    "    if atine_dense_layer == 0:\n",
    "        atine = layers.Dense(atine_dense_units, activation='elu')(stem)\n",
    "    else:\n",
    "        atine = layers.Dense(atine_dense_units, activation='elu')(atine)\n",
    "\n",
    "#### output\n",
    "atine_out = layers.Dense(int(len(astero_outputs)))(atine)\n",
    "\n",
    "######## construct and fit\n",
    "model = keras.Model(inputs=stem_input, outputs=[ctine_out, atine_out], name='tuning_fork')\n",
    "\n",
    "######## plot model\n",
    "keras.utils.plot_model(model, \"figs/\"+model_name+\".png\", show_layer_activations=True, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd160d-d8af-4e8e-b920-09a1eb2cfa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## compile and start training\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    \n",
    "model.compile(loss='MSE', optimizer=optimizer)\n",
    "\n",
    "#### fit model\n",
    "## learning rate scheduler, decreases learning rate in-training for stability\n",
    "def scheduler(epoch, lr):\n",
    "    if lr < 1e-5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.00006)\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=0)\n",
    "\n",
    "history = model.fit(df_train_inputs,\n",
    "                    [df_train_outputs[classical_outputs],df_train_outputs[astero_outputs]],\n",
    "                    validation_data=(df_val_inputs,[df_val_outputs[classical_outputs], df_val_outputs[astero_outputs]]),\n",
    "                    batch_size=32768,\n",
    "                    verbose=1,\n",
    "                    epochs=100000,\n",
    "                    callbacks=lr_callback,\n",
    "                    shuffle=True\n",
    "                   ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcdcbac-bada-4856-a2a5-c842c514b19a",
   "metadata": {},
   "source": [
    "## testing the network\n",
    "this section is very rushed, the main thing is to remember to rescale your data (as the network will predict in the normalised space), and to predict on the test set that was taken out of the dataset before training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afa699-0ac0-4908-89dd-435f39d9b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnorm(df_norm, df_min, df_max):\n",
    "    df_unnorm = (df_norm*(df_max-df_min)) + df_min\n",
    "    return df_unnorm\n",
    "\n",
    "preds_norm = model(np.array(df_test[inputs]))\n",
    "\n",
    "df_preds_norm = pd.DataFrame(preds, columns=outputs)\n",
    "df_preds_unnorm = unnorm(preds_df_norm, df[outputs].min(), df[outputs].max())\n",
    "\n",
    "df_test_unnorm = unnorm(df_test, df.min(), df.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f713e64a-2e5a-4425-900b-68bee7923258",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = corner.corner(df_test_unnorm[outputs])\n",
    "corner.corner(df_preds_unnorm, color='red', fig=figure);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
